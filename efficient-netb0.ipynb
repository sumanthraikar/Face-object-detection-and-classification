{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7129632,"sourceType":"datasetVersion","datasetId":4113305},{"sourceId":7136071,"sourceType":"datasetVersion","datasetId":4117742},{"sourceId":7138361,"sourceType":"datasetVersion","datasetId":4119538},{"sourceId":7159388,"sourceType":"datasetVersion","datasetId":4134938}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nimport torch\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision.transforms.functional as FT\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\nimport torch \nimport torch.nn as nn\nfrom math import ceil\n\nDEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T17:45:43.343067Z","iopub.execute_input":"2023-12-10T17:45:43.343334Z","iopub.status.idle":"2023-12-10T17:45:49.895657Z","shell.execute_reply.started":"2023-12-10T17:45:43.343309Z","shell.execute_reply":"2023-12-10T17:45:49.894816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Face_dataset(Dataset):\n    \n    def __init__(self, image_dirs, transforms=None):\n        self.num_classes = len(image_dirs)\n        image_paths = []\n        labels = []\n        for i in range(self.num_classes):\n            image_paths.extend([os.path.join(image_dirs[i], j) for j in os.listdir(image_dirs[i])])\n            labels.extend([i]* len(os.listdir(image_dirs[i])))\n#         for i in range(self.num_classes):\n#             image_paths.extend([os.path.abspath(j) for j in os.listdir(image_dirs[i])])\n#             labels.extend([i]* len(os.listdir(image_dirs[i])))\n#         for i in range(self.num_classes):\n#             image_paths.extend(os.listdir(image_dirs[i]))\n#             labels.extend([i]* len(os.listdir(image_dirs[i])))\n        self.image_dirs = image_paths\n        self.labels = labels\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_dirs)\n        \n    def __getitem__(self, index):\n        image = Image.open(self.image_dirs[index])\n        label = self.labels[index]\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n        \n        return image, label\n    \nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n\n        return img\n    \ndef get_loader(weighted_sampling, train_split = 0.7):\n    face_dir = '/kaggle/input/face-classify-v2/TRAINV2/FACES'\n    face_mask_dir = '/kaggle/input/face-classify-v2/TRAINV2/MASKED'\n    face_hat_dir = '/kaggle/input/face-classify-v2/TRAINV2/HAT'\n    face_mask_sunglass_dir = '/kaggle/input/face-classify-v2/TRAINV2/MASKwithSUNGLASS'\n    face_sunglass_dir = '/kaggle/input/face-classify-v2/TRAINV2/SUNGLASSES'\n    \n    BATCH_SIZE=10\n    \n    test_face_dir = '/kaggle/input/face-classify-test-data/TEST/FACE'\n    test_face_mask_dir = '/kaggle/input/face-classify-test-data/TEST/MASK'\n    test_face_hat_dir = '/kaggle/input/face-classify-test-data/TEST/HAT'\n    test_face_mask_sunglass_dir = '/kaggle/input/face-classify-test-data/TEST/mask_sunglasses'\n    test_face_sunglass_dir = '/kaggle/input/face-classify-test-data/TEST/SUNGLASSES'\n    \n    img_dirs = [face_dir,face_mask_dir,face_hat_dir,face_mask_sunglass_dir,face_sunglass_dir]\n    test_img_dirs = [test_face_dir,test_face_mask_dir,test_face_hat_dir,test_face_mask_sunglass_dir,test_face_sunglass_dir]\n\n    class_weights = [1,5578/2138,5578/363,5578/567,5578/2483]\n    transform = Compose([transforms.Resize((224, 224)), transforms.ToTensor(),])\n    dt = Face_dataset(img_dirs, transforms=transform)\n    test_dt = Face_dataset(test_img_dirs, transforms=transform)\n    \n    sample_weights = [0]*len(dt)\n    for idx, (data,label) in enumerate(dt):\n        class_weight = class_weights[int(label)]\n        sample_weights[idx] = class_weight\n    \n    if weighted_sampling:\n        sampler = WeightedRandomSampler(sample_weights,num_samples = len(sample_weights), replacement=True)\n        dataloader = DataLoader(dt,batch_size = BATCH_SIZE, sampler=sampler)\n    \n    else:\n        train_size = int(train_split * len(dt))\n        test_size = len(dt) - train_size\n        trains_dataset, stest_dataset = torch.utils.data.random_split(dt, [train_size, test_size])\n#         train_data_size = int(len(dt)*train_split)\n        train_dataloader = DataLoader(trains_dataset, batch_size = BATCH_SIZE,shuffle=True)\n        s_test_dataloader = DataLoader(stest_dataset, batch_size = BATCH_SIZE,shuffle=True)\n        \n    test_dataloader = DataLoader(test_dt, batch_size=BATCH_SIZE)\n    \n    return train_dataloader, s_test_dataloader, test_dataloader\n    \n    \n\n                ","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:45:49.897189Z","iopub.execute_input":"2023-12-10T17:45:49.897575Z","iopub.status.idle":"2023-12-10T17:45:49.914735Z","shell.execute_reply.started":"2023-12-10T17:45:49.897548Z","shell.execute_reply":"2023-12-10T17:45:49.913890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nbase_model = [\n    # expand_ratio, channels, repeats, stride, kernel_size\n    [1, 16, 1, 1, 3],\n    [6, 24, 2, 2, 3],\n    [6, 40, 2, 2, 5],\n    [6, 80, 3, 2, 3],\n    [6, 112, 3, 1, 5],\n    [6, 192, 4, 2, 5],\n    [6, 320, 1, 1, 3],\n]\n\nphi_values = {\n    # tuple of: (phi_value, resolution, drop_rate)\n    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n    \"b1\": (0.5, 240, 0.2),\n    \"b2\": (1, 260, 0.3),\n    \"b3\": (2, 300, 0.3),\n    \"b4\": (3, 380, 0.4),\n    \"b5\": (4, 456, 0.4),\n    \"b6\": (5, 528, 0.5),\n    \"b7\": (6, 600, 0.5),\n}\n\n\nclass CNNBlock(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n    ):\n        super(CNNBlock, self).__init__()\n        self.cnn = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            groups=groups,\n            bias=False,\n        )\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.silu = nn.SiLU()  # SiLU <-> Swish\n\n    def forward(self, x):\n        return self.silu(self.bn(self.cnn(x)))\n\n\nclass SqueezeExcitation(nn.Module):\n    def __init__(self, in_channels, reduced_dim):\n        super(SqueezeExcitation, self).__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),  # C x H x W -> C x 1 x 1\n            nn.Conv2d(in_channels, reduced_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(reduced_dim, in_channels, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return x * self.se(x)\n\n\nclass InvertedResidualBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        expand_ratio,\n        reduction=4,  # squeeze excitation\n        survival_prob=0.8,  # for stochastic depth\n    ):\n        super(InvertedResidualBlock, self).__init__()\n        self.survival_prob = 0.8\n        self.use_residual = in_channels == out_channels and stride == 1\n        hidden_dim = in_channels * expand_ratio\n        self.expand = in_channels != hidden_dim\n        reduced_dim = int(in_channels / reduction)\n\n        if self.expand:\n            self.expand_conv = CNNBlock(\n                in_channels,\n                hidden_dim,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            )\n\n        self.conv = nn.Sequential(\n            CNNBlock(\n                hidden_dim,\n                hidden_dim,\n                kernel_size,\n                stride,\n                padding,\n                groups=hidden_dim,\n            ),\n            SqueezeExcitation(hidden_dim, reduced_dim),\n            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n\n    def stochastic_depth(self, x):\n        if not self.training:\n            return x\n\n        binary_tensor = (\n            torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n        )\n        return torch.div(x, self.survival_prob) * binary_tensor\n\n    def forward(self, inputs):\n        x = self.expand_conv(inputs) if self.expand else inputs\n\n        if self.use_residual:\n            return self.stochastic_depth(self.conv(x)) + inputs\n        else:\n            return self.conv(x)\n\n\nclass EfficientNet(nn.Module):\n    def __init__(self, version, num_classes):\n        super(EfficientNet, self).__init__()\n        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n        last_channels = ceil(1280 * width_factor)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.features = self.create_features(width_factor, depth_factor, last_channels)\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(last_channels, num_classes),\n        )\n\n    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n        phi, res, drop_rate = phi_values[version]\n        depth_factor = alpha**phi\n        width_factor = beta**phi\n        return width_factor, depth_factor, drop_rate\n\n    def create_features(self, width_factor, depth_factor, last_channels):\n        channels = int(32 * width_factor)\n        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n        in_channels = channels\n\n        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n            out_channels = 4 * ceil(int(channels * width_factor) / 4)\n            layers_repeats = ceil(repeats * depth_factor)\n\n            for layer in range(layers_repeats):\n                features.append(\n                    InvertedResidualBlock(\n                        in_channels,\n                        out_channels,\n                        expand_ratio=expand_ratio,\n                        stride=stride if layer == 0 else 1,\n                        kernel_size=kernel_size,\n                        padding=kernel_size // 2,  # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n                    )\n                )\n                in_channels = out_channels\n\n        features.append(\n            CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n        )\n\n        return nn.Sequential(*features)\n\n    def forward(self, x):\n        x = self.pool(self.features(x))\n        return self.classifier(x.view(x.shape[0], -1))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:45:49.915980Z","iopub.execute_input":"2023-12-10T17:45:49.916296Z","iopub.status.idle":"2023-12-10T17:45:49.943558Z","shell.execute_reply.started":"2023-12-10T17:45:49.916270Z","shell.execute_reply":"2023-12-10T17:45:49.942759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True)\n    mean_loss = []\n    \n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        \n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # update progress bar\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n    \ndef test_fn(test_loader, model):\n    model.eval()\n    loop = tqdm(test_loader, leave = True)\n    accuracy=0.0\n    for batch_idx, (x,y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        preds = torch.argmax(out,1)\n        acc = torch.eq(preds,y).float()\n        \n        accuracy+=torch.mean(acc)\n    return accuracy/(batch_idx+1)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:45:49.946098Z","iopub.execute_input":"2023-12-10T17:45:49.946420Z","iopub.status.idle":"2023-12-10T17:45:49.961633Z","shell.execute_reply.started":"2023-12-10T17:45:49.946388Z","shell.execute_reply":"2023-12-10T17:45:49.960885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\n\nmodel = EfficientNet(version=\"b0\", num_classes=5).to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\ntrain_loader, stest_loader, real_testloader = get_loader(weighted_sampling=False)\n\nfor i in range(epochs):\n    train_fn(train_loader, model, optimizer, criterion)\n\naccuracy = test_fn(stest_loader, model)\nprint(accuracy.item())\n\naccuracy = test_fn(real_testloader, model)\nprint(accuracy.item())","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:45:49.962602Z","iopub.execute_input":"2023-12-10T17:45:49.962911Z","iopub.status.idle":"2023-12-10T18:01:02.115249Z","shell.execute_reply.started":"2023-12-10T17:45:49.962872Z","shell.execute_reply":"2023-12-10T18:01:02.114325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n# import matplotlib.pyplot as plt\n# def predictions(dataloader, model):\n#     model.eval()\n#     imgs=[]\n#     labels =[]\n# #     loop = tqdm(test_loader, leave = True)\n#     accuracy=0.0\n#     for batch_idx, (x,y) in enumerate(test_loader):\n#         x, y = x.to(DEVICE), y.to(DEVICE)\n#         out = model(x)\n#         preds = torch.argmax(out,1)\n#         for j in range(9):\n#             print(y[j].cpu())\n#             transform = transforms.ToPILImage()\n#             img = transform(x[j].cpu().detach())\n#             imgs.append(img)\n#             labels.append(preds[j].cpu())\n            \n#     return imgs, labels\n# #             plt.imsave(f'/kaggle/working/id{batch_idx}j{j}.png', img)\n# #             import time\n# #             time.sleep(5)\n# #             plt.imshow(x[j].cpu().detach().numpy())\nfrom PIL import Image\nimport time\nimport matplotlib.pyplot as plt\ndef predictions(dataloader, model):\n    model.eval()\n    imgs=[]\n    labels =[]\n#     loop = tqdm(test_loader, leave = True)\n    accuracy=0.0\n    for batch_idx, (x,y) in enumerate(dataloader):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        preds = torch.argmax(out,1)\n        start_time = time.time()\n        for j in range(9):\n            print(y[j].cpu())\n            transform = transforms.ToPILImage()\n            img = transform(x[j].cpu().detach())\n            imgs.append(img)\n            labels.append(preds[j].cpu())\n        end_time = time.time()\n        print(f'Time taken for {len(dataloader)} batchframes : {end_time-start_time}')\n    return imgs, labels\n#             plt.imsave(f'/kaggle/working/id{batch_idx}j{j}.png', img)\n#             import time\n#             time.sleep(5)\n#             plt.imshow(x[j].cpu().detach().numpy())\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:01:02.116804Z","iopub.execute_input":"2023-12-10T18:01:02.117130Z","iopub.status.idle":"2023-12-10T18:01:02.126690Z","shell.execute_reply.started":"2023-12-10T18:01:02.117104Z","shell.execute_reply":"2023-12-10T18:01:02.125778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs, labels = predictions(stest_loader, model)\nresults = '/kaggle/working/classify_results'\nif not os.path.exists(results):\n    os.makedirs(results)\nimport cv2\nclasses = ['FACE', 'MASK', 'HAT', 'MASK+SUNGLASS', 'SUNGLASS']\nfor i in range(len(labels)):\n    im = cv2.putText(np.array(imgs[i]), classes[int(labels[i].item())], (50,170), cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 0, 0),2,cv2.LINE_AA)\n    plt.imsave(os.path.join(results, f'fid{i}.png'), im)\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:01:02.128027Z","iopub.execute_input":"2023-12-10T18:01:02.128479Z","iopub.status.idle":"2023-12-10T18:02:52.942912Z","shell.execute_reply.started":"2023-12-10T18:01:02.128444Z","shell.execute_reply":"2023-12-10T18:02:52.942051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, model_arch, path):\n    if not os.path.exists(f'/kaggle/working/saved_models/{model_arch}'):\n        os.makedirs(f'/kaggle/working/saved_models/{model_arch}')\n    torch.save(model.state_dict(), f\"/kaggle/working/saved_models/{model_arch}/{path}.pth\")\n    \nsave_model(model, model_arch=\"effnetb0\", path=\"effnetb0_train\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:02:52.944048Z","iopub.execute_input":"2023-12-10T18:02:52.944331Z","iopub.status.idle":"2023-12-10T18:02:53.094192Z","shell.execute_reply.started":"2023-12-10T18:02:52.944305Z","shell.execute_reply":"2023-12-10T18:02:53.093154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \ndef visualize_predictions(model, test_loader, model_arch, sampling_type, num_images=15, quantized=False):\n        if quantized:\n            device = \"cpu\"\n        else:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.eval()\n        \n        count=0\n        all_preds = []\n        all_labels = []\n        all_images = []\n\n        fig, axes = plt.subplots(4,5, figsize=(15,8))\n\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_images.extend(inputs.cpu().numpy())\n                count+=1\n                if count==num_images:\n                    break\n\n        for i in range(num_images):\n            image = all_images[i]\n            label = all_labels[i]\n            pred = all_preds[i]\n\n            row = i//5\n            col = i%5\n            ax = axes[row, col]\n\n            # Visualize the image with true label and predicted label\n            # plt.figure()\n            # plt.subplot(4,5, i+1)\n            \n            ax.imshow(np.transpose(image, (1, 2, 0)))\n            ax.axis('off')\n            ax.set_title(f\"L{label} : P{pred}\")\n            # plt.xlabel(f\"Pred: {pred}\")\n\n        fig.suptitle('Labels = [0:FACES, 1:MASK, 2:HAT, 3:MASKwithSUNGLASSES, 4:SUNGLASS]', fontsize=13)\n        plt.tight_layout()\n        plt.savefig(f\"/kaggle/working/saved_models/{model_arch}/{model_arch}_{sampling_type}_prediction_images.png\")\n        plt.show()\n\ndef save_model_q(model, model_arch, path, sampling_type=None):\n    # save with script\n    torch.jit.save(torch.jit.script(model), f\"/kaggle/working/saved_models/{model_arch}/{path}_{sampling_type}.pth\")\n    \ndef evaluate_model(model, test_loader):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.eval()\n        \n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            start_time = time.time()\n            for inputs, labels in test_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n            end_time = time.time()\n        accuracy = accuracy_score(all_labels, all_preds)\n        time_taken = end_time - start_time\n        print(f\"Test Accuracy: {accuracy:.4f}\")\n        print(f'Time: {time_taken:.4f}')\n\n        return accuracy\n\ndef evaluate_qmodel(model, test_loader):\n        device = \"cpu\"\n        model.eval()\n        \n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            start_time = time.time()\n            for inputs, labels in test_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n        end_time = time.time()\n        accuracy = accuracy_score(all_labels, all_preds)\n        print(f\"Test Accuracy: {accuracy:.4f}\")\n        print(f'Took {end_time-start_time} secs for {len(test_loader)} loader points')\n\n        return accuracy\n\n ","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:02:53.095641Z","iopub.execute_input":"2023-12-10T18:02:53.096004Z","iopub.status.idle":"2023-12-10T18:02:53.115767Z","shell.execute_reply.started":"2023-12-10T18:02:53.095976Z","shell.execute_reply":"2023-12-10T18:02:53.114891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quantized_model (model_path, model_arch, label, dataloaders, sampling_type=None):\n    \n    model = EfficientNet(version=\"b0\", num_classes=5)\n\n        \n    model.load_state_dict(torch.load(model_path))\n    # new_m= copy.deepcopy(model)\n\n    model.eval()\n\n    qconfig = get_default_qconfig(\"x86\")\n    qconfig_mapping = QConfigMapping().set_global(qconfig)\n\n    example_inputs = torch.randn(5,3,224,224)\n\n    prepared_model = prepare_fx(model, qconfig_mapping, example_inputs)\n    print(prepared_model.graph)\n\n    def calibrate(model, data_loader):\n        with torch.inference_mode():\n            for image, target in data_loader:\n                model(image)\n    calibrate(prepared_model, dataloaders)  # run calibration on sample data\n\n    quantized_model = convert_fx(prepared_model)\n    print(quantized_model)\n\n\n    mname = f\"{model_arch}_{label}_FXGquant\"\n    save_model_q(quantized_model, model_arch, mname, sampling_type)\n    acc = evaluate_qmodel(quantized_model, dataloaders)\n    visualize_predictions(quantized_model, dataloaders,model_arch, sampling_type, 20, quantized=True)\n    \n#     r_acc = evaluate_qmodel(quantized_model, dataloaders)\n#     visualize_predictions(quantized_model, dataloaders,model_arch, sampling_type, 20, quantized=True)\n    print(\"#### Successfully saved quantized model ####\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:02:53.118861Z","iopub.execute_input":"2023-12-10T18:02:53.119132Z","iopub.status.idle":"2023-12-10T18:02:53.133479Z","shell.execute_reply.started":"2023-12-10T18:02:53.119107Z","shell.execute_reply":"2023-12-10T18:02:53.132765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.ao.quantization import get_default_qconfig\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\nfrom torch.ao.quantization import QConfigMapping\nfrom sklearn.metrics import accuracy_score\nquantized_model('/kaggle/working/saved_models/effnetb0/effnetb0_train.pth', \"effnetb0\", \"TEST\", stest_loader)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:02:53.134457Z","iopub.execute_input":"2023-12-10T18:02:53.134718Z","iopub.status.idle":"2023-12-10T18:17:01.379263Z","shell.execute_reply.started":"2023-12-10T18:02:53.134693Z","shell.execute_reply":"2023-12-10T18:17:01.378295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r effnetb0_scratch_allresults.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:17:01.380795Z","iopub.execute_input":"2023-12-10T18:17:01.381095Z","iopub.status.idle":"2023-12-10T18:17:12.751480Z","shell.execute_reply.started":"2023-12-10T18:17:01.381068Z","shell.execute_reply":"2023-12-10T18:17:12.750364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def quantized_model (model_path, model_arch, label, dataloaders, sampling_type):\n    \n#     model = ResNet_18(image_channels=3, num_classes=5)\n\n        \n#     model.load_state_dict(torch.load(model_path))\n#     # new_m= copy.deepcopy(model)\n\n#     model.eval()\n\n#     qconfig = get_default_qconfig(\"x86\")\n#     qconfig_mapping = QConfigMapping().set_global(qconfig)\n\n#     example_inputs = torch.randn(5,3,224,224)\n\n#     prepared_model = prepare_fx(model, qconfig_mapping, example_inputs)\n#     print(prepared_model.graph)\n\n#     def calibrate(model, data_loader):\n#         with torch.inference_mode():\n#             for image, target in data_loader:\n#                 model(image)\n#     calibrate(prepared_model, dataloaders['val'])  # run calibration on sample data\n\n#     quantized_model = convert_fx(prepared_model)\n#     print(quantized_model)\n\n\n#     mname = f\"{model_arch}_{label}_FXGquant\"\n#     save_model_q(quantized_model, model_arch, mname, sampling_type)\n#     acc = evaluate_qmodel(quantized_model, dataloaders['test'])\n#     visualize_predictions(quantized_model, dataloaders['test'],model_arch, sampling_type, 20, quantized=True)\n    \n#     r_acc = evaluate_qmodel(quantized_model, dataloaders['real_test'])\n#     visualize_predictions(quantized_model, dataloaders['real_test'],model_arch, sampling_type, 20, quantized=True)\n#     print(\"#### Successfully saved quantized model ####\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:17:12.753223Z","iopub.execute_input":"2023-12-10T18:17:12.753615Z","iopub.status.idle":"2023-12-10T18:17:12.759717Z","shell.execute_reply.started":"2023-12-10T18:17:12.753574Z","shell.execute_reply":"2023-12-10T18:17:12.758785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results = '/kaggle/working/classify_results'\n# if not os.path.exists(results):\n#     os.makedirs(results)\n# import cv2\n# classes = ['FACE', 'MASK', 'HAT', 'MASK+SUNGLASS', 'SUNGLASS']\n# for i in range(len(labels)):\n#     im = cv2.putText(np.array(imgs[i]), classes[int(labels[i].item())], (50,170), cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 0, 0),2,cv2.LINE_AA)\n#     plt.imsave(os.path.join(results, f'fid{i}.png'), im)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:17:12.761047Z","iopub.execute_input":"2023-12-10T18:17:12.761391Z","iopub.status.idle":"2023-12-10T18:17:12.775719Z","shell.execute_reply.started":"2023-12-10T18:17:12.761357Z","shell.execute_reply":"2023-12-10T18:17:12.774890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint = {\n#            \"state_dict\": model.state_dict(),\n#            \"optimizer\": optimizer.state_dict(),\n#        }\n# print('saving model checkpoint')\n# torch.save(checkpoint, 'resnet18_faceclassify.pth.tar')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:17:12.777016Z","iopub.execute_input":"2023-12-10T18:17:12.777269Z","iopub.status.idle":"2023-12-10T18:17:12.787544Z","shell.execute_reply.started":"2023-12-10T18:17:12.777246Z","shell.execute_reply":"2023-12-10T18:17:12.786591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_scripted = torch.jit.script(model.eval())\n# model_scripted.save('model_scripted.pt')\n# torch.onnx.export(model, torch.zeros((1, 3, 224, 224)), 'effnetb0.onnx', opset_version=14)\n\n# model_fp32_path = 'effnetb0_fp32.onnx'\n# dummy_in = torch.randn(1, 3, 224, 224, requires_grad=True)\n# torch.onnx.export(model,                                         # model\n#                   dummy_in,                                         # model input\n#                   model_fp32_path,                                  # path\n#                   export_params=True,                               # store the trained parameter weights inside the model file\n#                   opset_version=14,                                 # the ONNX version to export the model to\n#                   do_constant_folding=True,                         # constant folding for optimization\n#                   input_names = ['input'],                          # input names\n#                   output_names = ['output'],                        # output names\n#                   dynamic_axes={'input' : {0 : 'batch_size'},       # variable length axes\n#                                 'output' : {0 : 'batch_size'}})","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:17:12.788594Z","iopub.execute_input":"2023-12-10T18:17:12.788912Z","iopub.status.idle":"2023-12-10T18:17:12.810176Z","shell.execute_reply.started":"2023-12-10T18:17:12.788872Z","shell.execute_reply":"2023-12-10T18:17:12.809346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r effnetb0_allresults.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-12-10T18:17:12.811344Z","iopub.execute_input":"2023-12-10T18:17:12.811666Z","iopub.status.idle":"2023-12-10T18:17:12.823028Z","shell.execute_reply.started":"2023-12-10T18:17:12.811634Z","shell.execute_reply":"2023-12-10T18:17:12.822213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}